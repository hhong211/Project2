{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVSaPkYGhxgF"
      },
      "source": [
        "**Hypothesis:**\n",
        "The Perspective API will often misinterpret sarcasm and irony, scoring sarcastic or ironic comments as less toxic than straightforward toxic comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-imGqVjLNk2a"
      },
      "source": [
        "**Test cases combined with Ironic Comments and Straightforward Toxic Comments:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3Mk6KYVGtta",
        "outputId": "0b5b465e-26ec-415f-ad9a-84438694220e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the API's predictions compared to my predictions: 0.65\n"
          ]
        }
      ],
      "source": [
        "from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "API_KEY = 'my_api'\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")\n",
        "\n",
        "test_cases = [\n",
        "    {\"text\": \"Ah, traffic jams when you're already late. How wonderfully timed.\", \"my_label\": \"non-toxic\"},\n",
        "    {\"text\": \"I just love how my phone dies right when I need it the most.\", \"my_label\": \"non-toxic\"},\n",
        "    {\"text\": \"Great job on your report, it's not like facts are important anyway.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"Sure, take your time replying. It’s not like the house is on fire or anything.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"I absolutely adore how you state the obvious with such a sense of discovery.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"You crashed my car? Great, I was looking for a reason to walk to work every day.\", \"my_label\": \"non-toxic\"},\n",
        "    {\"text\": \"You forgot our anniversary again? You really have a talent for making this day memorable.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"You got a 25 on your exam? Well, at least you're consistent in disappointing us.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"Oh, fantastic, you're smoking inside the house again? I guess we needed the smell of burnt tobacco to cover up the lovely scent of fresh air.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"You've outdone yourself this time, a parking ticket and a towed car in one day? Should we celebrate your achievement?\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"You are a pathetic loser, and everyone knows it.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"Nobody likes you, you're a terrible person.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"Shut up, you idiot, nobody cares about your opinion.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"Get out of my sight, you're nothing but a waste of space.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"You must be the dumbest person I've ever met in my life.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"Your ideas are just as pathetic as you are.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"You'll never amount to anything, you're a born loser.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"I can't believe how stupid you are.\", \"my_label\": \"toxic\"},\n",
        "    {\"text\": \"Your personality is so bad, no wonder no one wants to be with you\", \"my_label\": \"toxic\"},\n",
<<<<<<< HEAD
        "    {\"text\": \"You're a disgrace, and you should be ashamed of yourself.\", \"my_label\": \"toxic\"}\n",
=======
        "    {\"text\": \"You're a disgrace, and you should be ashamed of yourself.\", \"my_label\": \"toxic\"},\n"
>>>>>>> 9650bcc8d75f2f9fbc8e7684899f63d62b37165e
        "]\n",
        "\n",
        "def get_toxicity_score(text):\n",
        "    analyze_request = {\n",
        "        'comment': {'text': text},\n",
        "        'requestedAttributes': {'TOXICITY': {}},\n",
        "        'languages': ['en']\n",
        "    }\n",
        "\n",
        "    response = client.comments().analyze(body=analyze_request).execute()\n",
        "    return response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
        "\n",
        "# Adding API's scores and labels to the dataset\n",
        "threshold = 0.5\n",
        "for test_case in test_cases:\n",
        "    score = get_toxicity_score(test_case[\"text\"])\n",
        "    test_case[\"api_score\"] = score\n",
        "    test_case[\"api_label\"] = \"toxic\" if score > threshold else \"non-toxic\"\n",
        "\n",
        "# Analyzing the accuracy based on my labels versus the API's labels\n",
        "correct_predictions = 0\n",
        "for test_case in test_cases:\n",
        "    if test_case[\"my_label\"] == test_case[\"api_label\"]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / len(test_cases)\n",
        "\n",
        "print(f\"Accuracy of the API's predictions compared to my predictions: {accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6PZDZR5OU47"
      },
      "source": [
        "**Transform to CSV File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m0W38I57uUM",
        "outputId": "8924f098-c933-46d6-91a8-5ac4098768c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 text   my_label  api_score  \\\n",
            "0   Ah, traffic jams when you're already late. How...  non-toxic   0.028739   \n",
            "1   I just love how my phone dies right when I nee...  non-toxic   0.112540   \n",
            "2   Great job on your report, it's not like facts ...      toxic   0.012440   \n",
            "3   Sure, take your time replying. It’s not like t...      toxic   0.042657   \n",
            "4   I absolutely adore how you state the obvious w...      toxic   0.020106   \n",
            "5   You crashed my car? Great, I was looking for a...  non-toxic   0.100023   \n",
            "6   You forgot our anniversary again? You really h...      toxic   0.020231   \n",
            "7   You got a 25 on your exam? Well, at least you'...      toxic   0.156562   \n",
            "8   Oh, fantastic, you're smoking inside the house...      toxic   0.151587   \n",
            "9   You've outdone yourself this time, a parking t...      toxic   0.034984   \n",
            "10   You are a pathetic loser, and everyone knows it.      toxic   0.853334   \n",
            "11        Nobody likes you, you're a terrible person.      toxic   0.736634   \n",
            "12  Shut up, you idiot, nobody cares about your op...      toxic   0.928801   \n",
            "13  Get out of my sight, you're nothing but a wast...      toxic   0.612239   \n",
            "14  You must be the dumbest person I've ever met i...      toxic   0.836470   \n",
            "15        Your ideas are just as pathetic as you are.      toxic   0.787120   \n",
            "16  You'll never amount to anything, you're a born...      toxic   0.765238   \n",
            "17                I can't believe how stupid you are.      toxic   0.911035   \n",
            "18  Your personality is so bad, no wonder no one w...      toxic   0.577953   \n",
            "19  You're a disgrace, and you should be ashamed o...      toxic   0.749544   \n",
            "\n",
            "    api_label  \n",
            "0   non-toxic  \n",
            "1   non-toxic  \n",
            "2   non-toxic  \n",
            "3   non-toxic  \n",
            "4   non-toxic  \n",
            "5   non-toxic  \n",
            "6   non-toxic  \n",
            "7   non-toxic  \n",
            "8   non-toxic  \n",
            "9   non-toxic  \n",
            "10      toxic  \n",
            "11      toxic  \n",
            "12      toxic  \n",
            "13      toxic  \n",
            "14      toxic  \n",
            "15      toxic  \n",
            "16      toxic  \n",
            "17      toxic  \n",
            "18      toxic  \n",
            "19      toxic  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(test_cases)\n",
        "\n",
        "print(df)\n",
        "\n",
        "df.to_csv('my_toxicity_dataset.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec8UBD-HPrxt"
      },
      "source": [
        "**Results:**\n",
        "\n",
        "Based on the results, the toxicity scores of ironic comments are very low. Some of them I think sound a little bit like a toxic comment and I guess it could be higher than 0.5 but it didn’t.  However, four ironic comments got a score higher than 0.1, which means they were seen as slightly toxic. For example, the comment about the phone dying at the worst possible time got a high score. This might be because the system thought the word \"dies\" was negative, as it's often used in bad situations. Also, the comment \"crashed my car?\" could have been marked as toxic because crashes are usually bad news. The comment about getting a 25 on an exam and always being a disappointment probably got a high score because being called \"disappointing\" is not a nice thing. Lastly, the comment about smoking inside might have been seen as more toxic because it talks about \"smoking\" and \"burnt tobacco,\" which are related to negative health effects. Although I didn't expect the model can detect non-obvious sarcasm. Like the comment \"Great job on your report, it's not like facts are important anyway.\" which I think meaning seems to be an obvious sarcastic comment, its toxicity score is only 0.012.\n",
        "\n",
        "In contrast, the comments with straightforwardly toxic garner significantly higher toxicity scores. All of these comments have toxicity scores higher than 0.5. The comment 'Shut up, you idiot, nobody cares about your opinion.' got the highest score of 0.9288, probably because it combines a direct insult with a complete dismissal of the individual’s viewpoint. The comment 'You are a pathetic loser, and everyone knows it.' got a third-highest score of 0.8533,  likely due to the direct and unambiguous insults it contains. The remaining comments all contain straightforward personal attacks.\n",
        "\n",
        "These results indicate that the model we use to measure toxicity in comments can only pick up on negative words and mark comments as toxic because of them. This happens even if the comments are meant to be funny or sarcastic. The problem is that this model could not understand when the meaning changes because of how something is said or the situation it's said. This shows that the model isn't perfect and doesn't completely get the way humans talk to each other, especially when it comes to sarcasm or irony."
      ]
    }
<<<<<<< HEAD
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
=======
  ]
}
>>>>>>> 9650bcc8d75f2f9fbc8e7684899f63d62b37165e
